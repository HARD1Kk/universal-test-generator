{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Case Generator\n",
    "Generate unit tests for any programming language using AI models.\n",
    "Works on both local Jupyter and Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment detection and setup\n",
    "import sys\n",
    "\n",
    "# Detect if running in Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print('Running in Google Colab')\n",
    "    # Install dependencies\n",
    "    !pip install -q openai anthropic google-generativeai transformers gradio python-dotenv accelerate bitsandbytes\n",
    "else:\n",
    "    print('Running locally')\n",
    "\n",
    "print('Environment detected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import torch\n",
    "import requests\n",
    "import subprocess\n",
    "\n",
    "# Frontier models\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Open source\n",
    "if not IN_COLAB:\n",
    "    import ollama\n",
    "else:\n",
    "    ollama = None\n",
    "\n",
    "try:\n",
    "    import groq\n",
    "except ImportError:\n",
    "    groq = None\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "\n",
    "# UI\n",
    "import gradio as gr\n",
    "\n",
    "print('Imports loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API keys based on environment\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Use Colab secrets\n",
    "    from google.colab import userdata\n",
    "    \n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n",
    "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "    HUGGINGFACE_API_KEY = userdata.get('HUGGINGFACE_API_KEY')\n",
    "    GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
    "    print('Using Colab secrets')\n",
    "else:\n",
    "    # Use .env file\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    \n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "    ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')\n",
    "    GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "    HUGGINGFACE_API_KEY = os.getenv('HUGGINGFACE_API_KEY')\n",
    "    GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
    "    print('Using .env file')\n",
    "\n",
    "# Initialize clients\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None\n",
    "anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY) if ANTHROPIC_API_KEY else None\n",
    "if GOOGLE_API_KEY:\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU available: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    print('No GPU available, using CPU')\n",
    "\n",
    "print('Environment configured')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model catalog\n",
    "MODEL_TYPES = ['GPT', 'Claude', 'Gemini', 'HuggingFace']\n",
    "\n",
    "MODEL_NAMES = {\n",
    "    'GPT': ['gpt-4o-mini', 'gpt-4o', 'gpt-3.5-turbo'],\n",
    "    'Claude': ['claude-3-5-haiku-latest', 'claude-3-5-sonnet-latest'],\n",
    "    'Gemini': ['gemini-2.0-flash-exp', 'gemini-1.5-pro', 'gemini-1.5-flash'],\n",
    "    'HuggingFace': [\n",
    "        'meta-llama/Llama-3.1-3B-Instruct',\n",
    "        'microsoft/bitnet-b1.58-2B-4T',\n",
    "        'tiiuae/Falcon-E-3B-Instruct',\n",
    "        'Qwen/Qwen2.5-7B-Instruct'\n",
    "    ]\n",
    "}\n",
    "\n",
    "CATEGORY_TO_PROVIDER = {\n",
    "    'GPT': 'openai',\n",
    "    'Claude': 'anthropic',\n",
    "    'Gemini': 'google',\n",
    "    'HuggingFace': 'huggingface'\n",
    "}\n",
    "\n",
    "print('Model catalog ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt\n",
    "SYSTEM_PROMPT = (\n",
    "    'You are an expert test engineer. Generate runnable unit tests for the provided code. '\n",
    "    'Requirements:\\n'\n",
    "    '- Include original code FIRST, then unit tests\\n'\n",
    "    '- Add brief comments explaining each test\\n'\n",
    "    '- Output ONLY code (no markdown fences, no extra text)\\n'\n",
    "    '- Make it runnable as-is (include main/runner if needed)\\n'\n",
    "    '- Keep tests deterministic (no network/filesystem/external deps unless mocked)\\n'\n",
    "    '- Cover happy path, negative cases, and boundaries'\n",
    ")\n",
    "\n",
    "print('System prompt ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def resolve_provider_and_model(category, selected_model=None):\n",
    "    provider = CATEGORY_TO_PROVIDER.get(category)\n",
    "    if not provider:\n",
    "        raise ValueError(f'Unknown category: {category}')\n",
    "    model_list = MODEL_NAMES.get(category, [])\n",
    "    model = selected_model if selected_model else (model_list[0] if model_list else None)\n",
    "    return provider, model\n",
    "\n",
    "def extract_code_and_lang(text):\n",
    "    text = text.strip()\n",
    "    match = re.match(r'^```(\\w+)?\\s*\\n([\\s\\S]*?)\\n```$', text, re.MULTILINE)\n",
    "    if match:\n",
    "        lang = (match.group(1) or '').lower()\n",
    "        code = match.group(2)\n",
    "        return code, lang if lang else None\n",
    "    return text, None\n",
    "\n",
    "def quant_config():\n",
    "    try:\n",
    "        return BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_quant_type='nf4'\n",
    "        )\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "print('Helpers ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provider response functions\n",
    "\n",
    "def get_chatgpt_response(prompt, model_name, output_tokens=1024):\n",
    "    if not openai_client:\n",
    "        raise RuntimeError('OpenAI API key not configured')\n",
    "    messages = [{'role': 'system', 'content': SYSTEM_PROMPT}, {'role': 'user', 'content': prompt}]\n",
    "    resp = openai_client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        max_tokens=output_tokens\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "def get_claude_response(prompt, model_name, output_tokens=1024):\n",
    "    if not anthropic_client:\n",
    "        raise RuntimeError('Anthropic API key not configured')\n",
    "    resp = anthropic_client.messages.create(\n",
    "        model=model_name,\n",
    "        max_tokens=output_tokens,\n",
    "        system=SYSTEM_PROMPT,\n",
    "        messages=[{'role': 'user', 'content': prompt}]\n",
    "    )\n",
    "    return resp.content[0].text\n",
    "\n",
    "def get_gemini_response(prompt, model_name, output_tokens=1024):\n",
    "    if not GOOGLE_API_KEY:\n",
    "        raise RuntimeError('Google API key not configured')\n",
    "    model = genai.GenerativeModel(model_name=model_name, system_instruction=SYSTEM_PROMPT)\n",
    "    resp = model.generate_content(\n",
    "        prompt,\n",
    "        generation_config={'max_output_tokens': output_tokens, 'temperature': 0.7}\n",
    "    )\n",
    "    return resp.text\n",
    "\n",
    "def get_hf_response(prompt, model_name, output_tokens=1024):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if not tokenizer.pad_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    full_prompt = f'{SYSTEM_PROMPT}\\n\\n{prompt}'\n",
    "    try:\n",
    "        inputs = tokenizer.apply_chat_template([{'role': 'user', 'content': full_prompt}], return_tensors='pt')\n",
    "    except:\n",
    "        inputs = tokenizer(full_prompt, return_tensors='pt')['input_ids']\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.to('cuda')\n",
    "    \n",
    "    if 'Falcon' in model_name:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', torch_dtype=torch.float16)\n",
    "    elif 'bitnet' in model_name:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', trust_remote_code=True)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', quantization_config=quant_config())\n",
    "    \n",
    "    outputs = model.generate(inputs, max_new_tokens=output_tokens)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    del model, tokenizer, inputs, outputs\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print('Providers ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio UI\n",
    "\n",
    "def predict(category, model, prompt, num_tests, output_tokens):\n",
    "    provider, resolved_model = resolve_provider_and_model(category, model)\n",
    "    final_prompt = f'Generate {int(num_tests)} unit tests.\\n\\nUser requirements:\\n{prompt}'\n",
    "    \n",
    "    if provider == 'openai':\n",
    "        response = get_chatgpt_response(final_prompt, resolved_model, output_tokens)\n",
    "    elif provider == 'anthropic':\n",
    "        response = get_claude_response(final_prompt, resolved_model, output_tokens)\n",
    "    elif provider == 'google':\n",
    "        response = get_gemini_response(final_prompt, resolved_model, output_tokens)\n",
    "    elif provider == 'huggingface':\n",
    "        response = get_hf_response(final_prompt, resolved_model, output_tokens)\n",
    "    else:\n",
    "        return gr.update(value='Unsupported provider', language=None)\n",
    "    \n",
    "    code, lang = extract_code_and_lang(response)\n",
    "    return gr.update(value=code, language=lang)\n",
    "\n",
    "with gr.Blocks(title='Test Case Generator') as demo:\n",
    "    gr.Markdown('# Test Case Generator')\n",
    "    gr.Markdown('Generate unit tests for any code using AI models')\n",
    "    \n",
    "    with gr.Row():\n",
    "        category_dd = gr.Dropdown(choices=MODEL_TYPES, value='GPT', label='Model Category')\n",
    "        model_dd = gr.Dropdown(choices=MODEL_NAMES['GPT'], label='Model', value=MODEL_NAMES['GPT'][0])\n",
    "    \n",
    "    with gr.Row():\n",
    "        num_tests_nb = gr.Number(value=5, precision=0, label='Number of tests', minimum=1, maximum=20)\n",
    "        tokens_sld = gr.Slider(minimum=128, maximum=4096, value=1024, step=128, label='Max output tokens')\n",
    "    \n",
    "    prompt_tb = gr.Textbox(lines=10, label='Your code / requirements', placeholder='Paste your code here...')\n",
    "    generate_btn = gr.Button('Generate Tests', variant='primary')\n",
    "    output_code = gr.Code(label='Generated Tests', language=None, lines=20)\n",
    "    \n",
    "    def on_category_change(cat):\n",
    "        models = MODEL_NAMES.get(cat, [])\n",
    "        return gr.update(choices=models, value=models[0] if models else None)\n",
    "    \n",
    "    category_dd.change(on_category_change, inputs=category_dd, outputs=model_dd)\n",
    "    \n",
    "    def on_generate(cat, mdl, prmpt, num, toks):\n",
    "        try:\n",
    "            return predict(cat, mdl, prmpt, num, toks)\n",
    "        except Exception as e:\n",
    "            return gr.update(value=f'Error: {str(e)}', language=None)\n",
    "    \n",
    "    generate_btn.click(\n",
    "        on_generate,\n",
    "        inputs=[category_dd, model_dd, prompt_tb, num_tests_nb, tokens_sld],\n",
    "        outputs=output_code\n",
    "    )\n",
    "\n",
    "print('UI ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch (auto-detects environment)\n",
    "if IN_COLAB:\n",
    "    demo.launch(share=True, debug=True)  # Public shareable link for Colab\n",
    "else:\n",
    "    demo.launch()  # Local server"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (virt_env)",
   "language": "python",
   "name": "virt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
